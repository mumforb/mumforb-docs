{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to mumforb docs","text":"<p>This is a compendium of hand-written documentation from various things @mumforb has come across at various times and places. </p>"},{"location":"application-frameworks/nestjs/glossary/","title":"Glossary","text":"<p>A collection of definitions that I found while perusing the Nest documentation. Placed here to try and keep everything straight:</p> <ul> <li>Glossary</li> <li>Controller:</li> <li>Provider:<ul> <li>Service:</li> <li>Repository:</li> <li>Factory:</li> <li>Helper:</li> </ul> </li> <li>Dependency Injection:</li> <li>Entity:</li> <li>DTO:</li> <li>Schema:</li> </ul>"},{"location":"application-frameworks/nestjs/glossary/#controller","title":"Controller:","text":"<p>Controllers should handle HTTP requests and delegate more complex tasks to providers.</p>"},{"location":"application-frameworks/nestjs/glossary/#provider","title":"Provider:","text":"<p>Providers are plain JavaScript classes that are declared as providers in a module. The main idea of a provider is that it can be injected as a dependency; this means objects can create various relationships with each other, and the function of \"wiring up\" these objects can largely be delegated to the Nest runtime system.</p> <p>Types of providers:</p>"},{"location":"application-frameworks/nestjs/glossary/#service","title":"Service:","text":"<p>Something responsible for a task, such as data storage and retreival. Used by the controller. Encapsulates the application's business logic. Can interact with databases, perform data processing, implement core functionality. Typically injected into controllers to handle HTTP requests.</p>"},{"location":"application-frameworks/nestjs/glossary/#repository","title":"Repository:","text":"<p>Encapsulate database operations. Often associated with ORM libraries. Provide abstraction layer for database interactions, like querying data.</p>"},{"location":"application-frameworks/nestjs/glossary/#factory","title":"Factory:","text":"<p>Used to create instances of classes or objects dynamically. Helpful when you need to generate objects with complex configurations or when you want to centralize creation of objects with specific dependencies.</p>"},{"location":"application-frameworks/nestjs/glossary/#helper","title":"Helper:","text":"<p>Encompass a broad category of providers that offer utility functions, reusable code, configurations. Not tied to a specific application layer.</p> <p>A LITTLE EXAMPLE: You might have a UserService (a service) that relies on UserRepository (a repository) to interact with the database. UserRepository may use a LoggerService (a helper) for for logging and a UserFactory (a factory) for creating user objects with specific configurations.</p>"},{"location":"application-frameworks/nestjs/glossary/#dependency-injection","title":"Dependency Injection:","text":"<p>Two main roles exist in the DI system: dependency consumer and dependency provider. When a dependency is requested, the injector checks its registry to see if there is an instance already available there. If not, a new instance is created and stored in the registry. Angular creates an application-wide injector (also known as \"root\" injector) during the application bootstrap process, as well as any other injectors as needed. In most cases you don't need to manually create injectors, but you should know that there is a layer that connects providers and consumers.</p>"},{"location":"application-frameworks/nestjs/glossary/#entity","title":"Entity:","text":"<p>Typically represents a data model or an object that can be stored and retrieved from a database. Always a typescript class. Can define both properties and relationships. Used with built-in decorators (@Entity or @Schema, depending on the database type), if appropriate.</p>"},{"location":"application-frameworks/nestjs/glossary/#dto","title":"DTO:","text":"<p>An object used to transfer data between parts of your application, such as between clients and server. It's a way to encapsulate and structure data when it's moving between components, helping to avoid transferring unnecessary data and ensuring the data transferred is in the expected format. So an entity and a DTO can look the same, but they don't necessarily look the same.</p>"},{"location":"application-frameworks/nestjs/glossary/#schema","title":"Schema:","text":"<p>Another built-in decorator, comparable to an Entity. It marks a class as a schema definition, for example to a MongoDB collection of the same name. (The Nest schema may be Cat, and the Mongo collection may be Cats.) This decorator accepts a single optional argument, the schema options object. </p>"},{"location":"application-frameworks/nestjs/syntax/","title":"Syntax","text":"<p>NestJS comes with a variety of syntactical sweeteners that were unusual at first glance. This is a partial inventory.</p>"},{"location":"application-frameworks/nestjs/syntax/#exclamation-marks-in-schemasentities","title":"Exclamation marks in Schemas/Entities","text":"<p>Used to correct a typescript error: <code>Property 'item' has no initializer and is not definitely assigned in the constructor.</code>  It comes into play when the Typescript <code>strictPropertyIntialization</code> flag is marked as true. This means that all properties \"should\" be initialized by the constructor. A quick workaround is to add the <code>!</code> postfix (e.g. <code>item!</code>), which asserts that this class property doesn't need to be explicitly initialized by a constructor.</p>"},{"location":"databases/cassandra/","title":"Cassandra","text":"<p>a nosql database known for availability and scalability particularly suitable for handing large amounts fo data across distributed commodity servers</p>"},{"location":"databases/dynamodb/","title":"DynamoDB","text":"<p>a managed nosql database service provided by AWS</p>"},{"location":"databases/mariadb/","title":"Mariadb","text":"<p>a fork of mysql, designed for open-source development. offers similar features to mysql </p>"},{"location":"databases/mysql/","title":"Mysql","text":"<p>open source ACID compliant and widely used in various applications.</p>"},{"location":"databases/neo4j/","title":"Neo4j","text":"<p>a graph database designed for storing and querying graph structures ideal for applications with complex relationships, such as social networks or recommendation systems</p>"},{"location":"databases/nosql/","title":"Nosql","text":"<p>mongodb (nosql) pros 1. flexible schema - easily adapt to changing data structures 2. scalability - scales horizontally, making it suitable for lorage-scale apps 3. json-like documents 4. quick development - no schema means it's faster to develo in</p> <p>cons 1. no joins - lack of support for traditional sql joins, which might impact complex querying 2. atomic transactions - limited support fo rmulti-document transactions compared to relational dbs 3. less mature - some features may not be available</p> <p>postgresql (relational) 1. ACID compliance - insures data integrity through atomicity, consistency, isolation, and durability 2. powerful queries - supports complex queries and joins, suitable for data-intensive applications 3. mature - well-established with a wide range of tools 4. relationships - easily define and manage relationships between tables</p> <p>cons 1. fixed schema - may require schema changes for evolving data structures 2. scaling challenges - vertical scaling is possible, horizontal scaling can be more complex 3. development speed - initial setup and development might take longer due to the need to define a structured schema</p> <p>NoSQL databases are often preferred for flexible, rapidly evolving projects, while relational database are chosen for scenarios demanding strict data integrity and structured relationships.</p>"},{"location":"databases/postgres/","title":"Postgres","text":""},{"location":"databases/redis/","title":"Redis","text":"<p>an in-memory data structure store often used as a caching mechanism, message broker, or for real-time analytics</p>"},{"location":"databases/relational-vs-nosql/","title":"Relational vs nosql","text":"<p>mongodb (nosql) pros 1. flexible schema - easily adapt to changing data structures 2. scalability - scales horizontally, making it suitable for lorage-scale apps 3. json-like documents 4. quick development - no schema means it's faster to develo in</p> <p>cons 1. no joins - lack of support for traditional sql joins, which might impact complex querying 2. atomic transactions - limited support fo rmulti-document transactions compared to relational dbs 3. less mature - some features may not be available</p> <p>postgresql (relational) 1. ACID compliance - insures data integrity through atomicity, consistency, isolation, and durability 2. powerful queries - supports complex queries and joins, suitable for data-intensive applications 3. mature - well-established with a wide range of tools 4. relationships - easily define and manage relationships between tables</p> <p>cons 1. fixed schema - may require schema changes for evolving data structures 2. scaling challenges - vertical scaling is possible, horizontal scaling can be more complex 3. development speed - initial setup and development might take longer due to the need to define a structured schema</p> <p>NoSQL databases are often preferrred for flexible, rapidly evolving projects, while relational database are chosen for scenarios demanding strict data integrity and structured relationships.</p>"},{"location":"databases/sql/","title":"SQL","text":""},{"location":"databases/sqlite/","title":"Sqlite","text":"<p>self-cointained, serverless, and zero configuration database suitable for small to medium-sized applications</p>"},{"location":"databases/mongodb/database-class/","title":"Mongo Database Class Methods","text":"<p>For reference, this is a copy/paste of everything that printed from calling <code>db.help()</code> inside <code>mongosh</code> while connected to a DB (v7.0.5):</p> <p>Database Class:</p> <pre><code>getMongo                                   Returns the current database connection\ngetName                                    Returns the name of the DB\ngetCollectionNames                         Returns an array containing the names of all collections in the current database.\ngetCollectionInfos                         Returns an array of documents with collection information, i.e. collection name and options, for the current database.\nrunCommand                                 Runs an arbitrary command on the database.\nadminCommand                               Runs an arbitrary command against the admin database.\naggregate                                  Runs a specified admin/diagnostic pipeline which does not require an underlying collection.\ngetSiblingDB                               Returns another database without modifying the db variable in the shell environment.\ngetCollection                              Returns a collection or a view object that is functionally equivalent to using the db.&lt;collectionName&gt;.\ndropDatabase                               Removes the current database, deleting the associated data files.\ncreateUser                                 Creates a new user for the database on which the method is run. db.createUser() returns a duplicate user error if the user already exists on the database.\nupdateUser                                 Updates the user\u2019s profile on the database on which you run the method. An update to a field completely replaces the previous field\u2019s values. This includes updates to the user\u2019s roles array.\nchangeUserPassword                         Updates a user\u2019s password. Run the method in the database where the user is defined, i.e. the database you created the user.\nlogout                                     Ends the current authentication session. This function has no effect if the current session is not authenticated.\ndropUser                                   Removes the user from the current database.\ndropAllUsers                               Removes all users from the current database.\nauth                                       Allows a user to authenticate to the database from within the shell.\ngrantRolesToUser                           Grants additional roles to a user.\nrevokeRolesFromUser                        Removes a one or more roles from a user on the current database.\ngetUser                                    Returns user information for a specified user. Run this method on the user\u2019s database. The user must exist on the database on which the method runs.\ngetUsers                                   Returns information for all the users in the database.\ncreateCollection                           Create new collection\ncreateEncryptedCollection                  Creates a new collection with a list of encrypted fields each with unique and auto-created data encryption keys (DEKs). This is a utility function that internally utilises ClientEnryption.createEncryptedCollection.\ncreateView                                 Create new view\ncreateRole                                 Creates a new role.\nupdateRole                                 Updates the role\u2019s profile on the database on which you run the method. An update to a field completely replaces the previous field\u2019s values.\ndropRole                                   Removes the role from the current database.\ndropAllRoles                               Removes all roles from the current database.\ngrantRolesToRole                           Grants additional roles to a role.\nrevokeRolesFromRole                        Removes a one or more roles from a role on the current database.\ngrantPrivilegesToRole                      Grants additional privileges to a role.\nrevokePrivilegesFromRole                   Removes a one or more privileges from a role on the current database.\ngetRole                                    Returns role information for a specified role. Run this method on the role\u2019s database. The role must exist on the database on which the method runs.\ngetRoles                                   Returns information for all the roles in the database.\ncurrentOp                                  Runs an aggregation using $currentOp operator. Returns a document that contains information on in-progress operations for the database instance. For further information, see $currentOp.\nkillOp                                     Calls the killOp command. Terminates an operation as specified by the operation ID. To find operations and their corresponding IDs, see $currentOp or db.currentOp().\nshutdownServer                             Calls the shutdown command. Shuts down the current mongod or mongos process cleanly and safely. You must issue the db.shutdownServer() operation against the admin database.\nfsyncLock                                  Calls the fsync command. Forces the mongod to flush all pending write operations to disk and locks the entire mongod instance to prevent additional writes until the user releases the lock with a corresponding db.fsyncUnlock() command.\nfsyncUnlock                                Calls the fsyncUnlock command. Reduces the lock taken by db.fsyncLock() on a mongod instance by 1.\nversion                                    returns the db version. uses the buildinfo command\nserverBits                                 returns the db serverBits. uses the buildInfo command\nisMaster                                   Calls the isMaster command\nhello                                      Calls the hello command\nserverBuildInfo                            returns the db serverBuildInfo. uses the buildInfo command\nserverStatus                               returns the server stats. uses the serverStatus command\nstats                                      returns the db stats. uses the dbStats command\nhostInfo                                   Calls the hostInfo command\nserverCmdLineOpts                          returns the db serverCmdLineOpts. uses the getCmdLineOpts command\nrotateCertificates                         Calls the rotateCertificates command\nprintCollectionStats                       Prints the collection.stats for each collection in the db.\ngetProfilingStatus                         returns the db getProfilingStatus. uses the profile command\nsetProfilingLevel                          returns the db setProfilingLevel. uses the profile command\nsetLogLevel                                returns the db setLogLevel. uses the setParameter command\ngetLogComponents                           returns the db getLogComponents. uses the getParameter command\ncloneDatabase                              deprecated, non-functional\ncloneCollection                            deprecated, non-functional\ncopyDatabase                               deprecated, non-functional\ncommandHelp                                returns the db commandHelp. uses the passed in command with help: true\nlistCommands                               Calls the listCommands command\ngetLastErrorObj                            Calls the getLastError command\ngetLastError                               Calls the getLastError command\nprintShardingStatus                        Calls sh.status(verbose)\nprintSecondaryReplicationInfo              Prints secondary replicaset information\ngetReplicationInfo                         Returns replication information\nprintReplicationInfo                       Formats sh.getReplicationInfo\nprintSlaveReplicationInfo                  DEPRECATED. Use db.printSecondaryReplicationInfo\nsetSecondaryOk                             This method is deprecated. Use db.getMongo().setReadPref() instead\nwatch                                      Opens a change stream cursor on the database\nsql                                        (Experimental) Runs a SQL query against Atlas Data Lake. Note: this is an experimental feature that may be subject to change in future releases.\ncheckMetadataConsistency                   Returns a cursor with information about metadata inconsistencies\n</code></pre>"},{"location":"databases/mongodb/mongodb/","title":"MongoDB","text":"<p>Connect to a mongodb instance at a URL, using a format like mongodb://localhost:27017</p> <p>Load initial data via built-in methods; it might look something like this:</p> <pre><code>const client = new MongoClient(url);\nconst db = client.db(dbName);\nresults = await db.collection('nameOfCollection').insertMany(data);\n</code></pre> <p>This will take a seeding file and put it in a specified database. That db object is the crux of a lot of the operations; creating an instance of the MongoClient is more or less step one. The above code snippet was demonstrated in a Promise.</p>"},{"location":"databases/mongodb/mongodb/#assert","title":"Assert","text":"<p>Demo I was watching used assert (like, the core testing library) to verify that the number of records inserted when initially setting up the db equalled the expected number. Wild!</p>"},{"location":"databases/mongodb/mongodb/#_id","title":"_id","text":"<p>The _id that's automatically generated is a special type - it isn't jsut a string, even though it looks like just a string. You can convert it from a string to the special type via Mongo.ObjectID(id).</p>"},{"location":"databases/mongodb/mongodb/#cursor","title":"Cursor","text":"<p>This is using things like .limit(), which is a marker that helps you do things that help do things in the data prior to it being permanently saved within the DB.</p>"},{"location":"databases/mongodb/mongodb/#built-in-methods","title":"Built in methods","text":"<p>get() - gets an array, even if you have a parameter as an argument (similar to find()) getById(id) - always returns one object (similar to findOne()). you need to use the _id that's built-in. insertMany(data) insertOne(data) findOneAndReplace({_id: ObjetID(id)}, {..data}, options) remove(id)</p> <p>And so on. Idea is that all the built-in methods extend right off the repository object, and allow you to do lots of the main tasks with no additional effort. Neato!</p> <p>Built in functions .serverStatus() and .listDatabases() are very useful to give lots of information on the mongo database itself. These are methods on client.db()dbName.admin().</p>"},{"location":"practices/code-quality/precommit/","title":"Pre-Commit","text":"<p>Pre-Commit is \"A framework for managing and maintaining multi-language pre-commit hooks.\"  Pre-commit hooks allow for checking of nearly unlimited number of items before a developer is allowed to successfully make a commit in their local development environment.  These checks can include:</p> <ul> <li> <p>Running static analyzers, including SAST tools</p> </li> <li> <p>Checking for secrets, private SSH keys, and other items that should not be uploaded to source control</p> </li> <li> <p>Housekeeping, including cleaning up excess whitespace or inconsistent line endings</p> </li> <li> <p>Other linting, including formatting of JSON and YAML files</p> </li> <li> <p>Custom testing, including checking for valid JIRA numbers within a commit.  These tests can be written by engineers in Python, Bash, or other scripting languages.</p> </li> </ul>"},{"location":"practices/code-quality/precommit/#pre-commit-package","title":"Pre-Commit package","text":"<p>Pre-commit is described as \"a framework for managing and maintaining multi-language pre-commit hooks.\"  Information about the pre-commit Python package can be found here.</p> <p>The pre-commit package must be installed:</p> <pre><code>$ pip install pre-commit\n</code></pre> <p>Pre-commit can be installed at the root of any project with:</p> <pre><code>$ pre-commit install\n</code></pre>"},{"location":"practices/code-quality/precommit/#terraform-technology-stack-packages","title":"Terraform technology stack packages","text":"<p>The Terraform stack packages listed below are open source when implemented with pre-commit, and do an excellent job of being complementary in their scans in order to be thorough in results when used in combination with each other.  The tools are checkov, kics, terraform-compliance, terrascan, tfsec, and tfupdate.</p>"},{"location":"practices/code-quality/precommit/#checkov","title":"checkov","text":"<p>Checkov by Bridgecrew is a static code analyzer to look for misconfigurations in code before the code is deployed.  This includes Terraform, Helm charts, Kubernetes, and Docker among others.  Checkov can be found here.  Checkov can be installed as a pip or brew package.  The brew package seems to run smoother and is recommended for installation:</p> <pre><code>$ brew install checkov\n</code></pre> <p>Checkov policies for Terraform can be found here.  Checkov individual checks or rile checks can be suppressed as found in this documentation.  Please note that comments are required by the DevOps team for rules that have been suppresed.</p>"},{"location":"practices/code-quality/precommit/#terraform-compliance","title":"terraform-compliance","text":"<p>Terraform Compliance is \"a lightweight, security and compliance focused test framework against Terraform to enable negative testing capability for your infrastructure-as-code.\"  Terraform Compliance can be installed using pip with the command:</p> <pre><code>$ pip install terraform-compliance\n</code></pre> <p>More information about Terraform Compliance can be found here.</p>"},{"location":"practices/code-quality/precommit/#terrascan","title":"terrascan","text":"<p>Terrascan is another static analysis security tool which helps to \"detect compliance and security violations across Infrastructure as Code (IaC) to mitigate risk before provisioning cloud native infrastructure\" and can be found here.</p> <p>Terrascan can be installed with the following command:</p> <pre><code>$ brew install terrascan\n</code></pre> <p>Terrascan policies for AWS can be found here.  Rule skipping documentation is available in this documentation.</p>"},{"location":"practices/code-quality/precommit/#tfsec","title":"tfsec","text":"<p>Tfsec is another static analysis scanner for Terraform code supported by Aqua Security and can be found here.  It uses OPA (Rego) to define policies as well.</p> <p>Tfsec can be installed with the following command:</p> <pre><code>$ brew install tfsec\n</code></pre> <p>TFSec policies can be found here.  Ignoring checks can be found with this documentation.</p>"},{"location":"practices/code-quality/precommit/#tfupdate","title":"tfupdate","text":"<p>TFUpdate is a utility to update version constraints of Terraform core, providers, and modules.  The utility can be found [here] (https://github.com/minamijoyo/tfupdate) and can be installed with the following command:</p> <pre><code>$ brew install tfupdate\n</code></pre>"},{"location":"practices/code-quality/precommit/#pre-commit-configuration-file","title":"Pre-Commit Configuration File","text":"<p>Below is an example of the .pre-commit-config.yaml file to be placed within a Terraform external module.  Additional tests for infrastructure repos that utilize external modules would be Infracost.</p> <pre><code>repos:\n- repo: https://github.com/gruntwork-io/pre-commit\n  rev: v0.1.17 # Get the latest from: https://github.com/gruntwork-io/pre-commit/releases\n  hooks:\n    - id: terraform-fmt\n    - id: terraform-validate\n    - id: tflint\n      args:\n        - \"--config=.tflint.hcl\"\n        - \"--enable-rule=terraform_documented_variables\"\n        - \"--module\"\n- repo: https://github.com/antonbabenko/pre-commit-terraform\n  rev: v1.74.1 # Get the latest from: https://github.com/antonbabenko/pre-commit-terraform/releases\n  hooks:\n    - id: tfupdate\n      name: Autoupdate Terraform versions\n    - id: tfupdate\n      name: Autoupdate AWS provider versions\n      args:\n      - --args=provider aws # Will be pined to latest version\n    - id: terraform_checkov\n    - id: terraform_docs\n    - id: terraform_tfsec\n    - id: terrascan\n      args: [\"\"]\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v4.3.0\n  hooks:\n    - id: check-added-large-files\n    - id: check-ast\n    - id: check-case-conflict\n    - id: check-executables-have-shebangs\n    - id: check-json\n    - id: check-merge-conflict\n    - id: check-shebang-scripts-are-executable\n    - id: check-symlinks\n    - id: check-toml\n    - id: check-xml\n    - id: check-yaml\n    - id: destroyed-symlinks\n    - id: detect-aws-credentials\n      args: [\"--allow-missing-credentials\"]\n    - id: detect-private-key\n    - id: end-of-file-fixer\n    - id: fix-byte-order-marker\n    - id: mixed-line-ending\n      args: [--fix=lf]\n    - id: pretty-format-json\n    - id: sort-simple-yaml\n    - id: trailing-whitespace\n</code></pre> <p>To update above referenced versions to be sure the latest is used, please use the command:</p> <pre><code>$ pre-commit autoupdate\n</code></pre> <p>Pre-commit can be test run with the command:</p> <pre><code>$ pre-commit run -a\n</code></pre>"},{"location":"practices/code-quality/terratest/","title":"Terratest","text":"<p>Terratest is a unit testing platform for Terraform files. It's a little different than the way we often think about unit tests, due to the fact that Terraform - and declarative IaC code in general - is different than other types of code. In order to test more granular bits of Terraform (e.g. modules) we can only validate the status of the execution by actually deploying real cloud components and checking to make sure they look like we expect. This is how we achieve a full \"test\" of the code, beyond what the linting and verification packages we use (like Terrascan, KICS, Checkov, and others).</p> <p>Because Terratest actually deploys real components (however briefly), it is important to keep the AWS account in which we're working separate from the main accounts that contain our actual \"stuff\". For now, we only have the capability of running Terratest locally (rather than in a CI/CD pipeline) due to our inability to safely distribute AWS login secrets into Gitlab, but this will change soon.</p>"},{"location":"practices/code-quality/terratest/#run-terratest-locally","title":"Run Terratest Locally","text":"<p>If you are using a VSCode <code>.devcontainer</code>, you may already have Go installed, which is a prerequisite for running Terratest. In fact, Terratest sits on top of Go's built-in testing platform, so most of the boilerplate setup involves getting Go up and running.</p> <p>Once Go is installed in your environment, you can execute the following steps, some of which may have already happened, but here's the full list for clarity.</p> <p>First, log into AWS. - Note that this step is required, even for non-AWS interacting IaC modules - UNLESS you do not need a secret housed in AWS Secrets Manager OR you supply the secret to your local environment via environment variable. The name for the variable can be found in the Terratest code itself.)</p> <ul> <li>$ aws sso login</li> </ul> <p>Then, make sure you select the AWS Test Integration account:</p> <ul> <li>$ aws configure list-profiles # just to see which profiles are available</li> <li>$ export AWS_PROFILE=test-integration</li> <li>$ aws configure list # verify the test-integration account is selected</li> </ul> <p>Assuming your test files are in a folder named <code>/test</code> (named something like <code>name_of_module_test.go</code>).</p> <ul> <li>$ cd test</li> <li>$ go mod init gitlab.com/this-is/your-repository # see note</li> <li>$ go mod tidy</li> </ul> <p>note     The <code>go mod</code> command initializes a Go module where the command is run. The module name is arbitrary but usually follows a format like github.com//. <p>warning     When attempting to follow this process in the EKS repo, the <code>go.mod</code> file didn't have anything in the \"required\" block, which I thought was strange. So I manually copied that over from this repo, and that then triggered the creation of a <code>go.sum</code> file, which in turn seemed to activate the dependencies. Not sure what the deal is with that.</p> <p>Now you should be ready to run the test.</p> <ul> <li>$ go test -v</li> </ul> <p>You'll see the terminal crank away for a while, as it is actually running <code>terraform init</code>, <code>terraform apply</code>, and eventually <code>terraform destroy</code> under the hood. There will be useful printout to the console to track what's happening. Additionally, if you log into AWS in the browser, you'll be able to see these assets briefly appear and disappear, which may help with troubleshooting your tests.</p>"},{"location":"practices/code-quality/terratest/#good-to-know","title":"Good To Know","text":"<ul> <li>Terratest does best when you test an entire module all at once - all the inputs and outputs, and whatever auxiliary properties can also be verified.</li> </ul>"},{"location":"practices/containerization/docker/bridge-networks/","title":"Bridge Networks","text":"<p>Bridge networks allow communication between containers. This allows any container that has access to the network to talk to each other. </p> <p><code>docker network create --driver bridge network-name</code> - this command creates a bridge network that allows communication between the containers. Happens at the command line using the Docker CLI. (In general, obviously, this will be set up via Docker Compose, but this just demonstrates the capability of doing it manually.)</p> <p><code>docker run -d -p 3000:3000 --net=network-name --name=mongodb mongo</code> - this runs a Docker image and attaches it to the listed network.</p> <p>Then you add additional containers to the same network one at a time, and making sure each container has a unique name. Then, by passing in config, you can sync up paths and hostnames, allowing for container-to-container communication that is predictable and scripted.</p>"},{"location":"practices/containerization/docker/docker-compose/","title":"Docker Compose","text":""},{"location":"practices/containerization/docker/docker-file/","title":"Dockerfile","text":"<p>A test document that contains all the commands a user could call on the command line.</p> <p>Dockerfile &gt;&gt; <code>docker build</code> &gt;&gt; Container</p> <p>Common commands in a Dockerfile.</p> <pre><code>FROM node:alpine\nLABEL author=\"Me\"\nENV NODE_ENV=production\nWORKDIR /var/www # you are here once the container starts running\nCOPY . . # where to copy from, and where to copy to, and launch\nRUN npm install # anything you want to run at launch\nEXPOSE 3000 # port number it's going to be listening on\nENTRYPOINT [\"node\", \"server.js\"] # first command to run to start the executable/starting point\n</code></pre>"},{"location":"practices/containerization/docker/docker-file/#tips","title":"Tips","text":"<p>Docker works with layers, so an efficient way to mount all the npm code into a Docker image might be:</p> <pre><code>...\nWORKDIR /var/www\nCOPY package*.json # wildcard gets both package.json and package-lock.json\nRUN npm ci\n\nCOPY . ./\n</code></pre> <p>This is particularly powerful along with a <code>.dockerignore</code>.</p>"},{"location":"practices/containerization/docker/docker-file/#deploy","title":"Deploy","text":"<p><code>docker build -t &lt;registry&gt;/&lt;name&gt;:&lt;version&gt; .</code> <code>docker push &lt;registry&gt;/&lt;name&gt;:&lt;version&gt;</code></p> <p>Versioning is really, really important.</p>"},{"location":"practices/containerization/docker/docker-file/#vs-code-docker-extension","title":"VS Code Docker Extension","text":""},{"location":"practices/containerization/docker/docker/","title":"Docker","text":"<p>Why Docker? A traditional shipping methodology might involve a lot of hope and work to sync up environments all the way through. If, however, you are able to specify the container in which the code itself runs, then it will really clean up and universalize every step along the way. It solves any kind of \"works on my machine\" type problems that happen between each environment, from development to production.</p> <p>It also gives the ability to totally redo environments from scratch quickly. It onboards devs more quickly as well, as their environments can be spun up reliably much more quickly.</p>"},{"location":"practices/containerization/docker/docker/#image","title":"Image","text":"<p>A layered file system including your code, server code, and everything else. Layered filesystems, a stack of books. An image, once run, is a container. Containers are the run-time version that can be run, started, stopped, moved, deleted, and re-run. Keep in mind that images are pegged to their version, and are therefore immutable. You don't change an image, you release/deploy a new one.</p> <p>Visualize layers in an image, like a layer cake. Some or all may be immutable. The container layer is a thin read/write layer on top of the immutable image layers below.</p> <p>So, to run a container with an open port accessible from the host: <code>docker run -p &lt;externalPort&gt;:&lt;internalPort&gt; &lt;imageName&gt;</code> <code>docker run -p &lt;externalPort&gt;:&lt;internalPort&gt; -d &lt;imageName&gt;</code> # this runs it without taking over the console</p>"},{"location":"practices/containerization/docker/docker/#troubleshooting","title":"Troubleshooting","text":"<p>docker logs"},{"location":"practices/containerization/docker/docker/#docker-desktop","title":"Docker Desktop","text":"<p>The default. Comes with everything, including a UI (at least in Windows and Mac).</p> <ul> <li>docker ps -a - list all running containers</li> <li>docker exec -it  sh # shell into a specific container (sh is the type of shell) <li>docker rmi  <li>docker images</li>"},{"location":"practices/containerization/docker/docker/#tips","title":"Tips","text":""},{"location":"practices/containerization/docker/docker/#-when-accessing-a-given-resource-for-example-a-container-you-only-have-to-paste-in-the-first-3-alphanumeric-characters-if-they-are-unique","title":"- When accessing a given resource (for example, a container) you only have to paste in the first 3 alphanumeric characters if they are unique.","text":""},{"location":"practices/containerization/docker/volumes/","title":"Volumes","text":"<p>If you have a container that is writing logs, for example, you may want to persist that. So, you can put these log files somewhere else.</p> <p><code>docker run -p &lt;ports&gt; -v $(pwd)/var/www/logs &lt;imageToRun&gt;</code></p>"},{"location":"practices/containerization/kubernetes/configmaps/","title":"ConfigMaps","text":"<p>Closely related to secrets. It's a way to store config information and provide it to containers. Used to inject config data in to a container. Can store entire files or provide key/value pairsL</p> <p>Filename becomes the key. Also can put them on the command-line. Also a yaml file as a \"manifest\".</p> <p>Access those values via env vars, or as a ConfigMap Volume (which you access as a file).</p>"},{"location":"practices/containerization/kubernetes/configmaps/#creating-a-configmap","title":"Creating a ConfigMap","text":"<p>A yaml example of some data registered inside a cluster via a ConfigMap.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-settings\n  labels:\n    app: app-settings\ndata:\n  enemies: aliens\n  lives: \"3\"\n  enemies.cheat: \"true\"\n  enemies.cheat.level: noGoodRotten\n  ```\n\n  kubectl create -f file.configmap.yml\n\n  OR\n\n  Just a recular file:\n\n  ```\nenemies=aliens\nlives=3\nenemies.cheat=true\nenemies.cheat.level=noGoodRotten\n</code></pre> <p>kubectl create configmap [cm-name] --from-file=[path-to-file]</p> <p>Becomes this yaml:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\ndata:\n    game.config: |-\n        enemies=aliens\n        lives=3\n        enemies.cheat=true\n        enemies.cheat.level=noGoodRotten\n</code></pre> <p>OR</p> <p>Can also use tha tregular file to create env variables:</p> <p>kubectl create configmap [cm-name] --from-env-file=[path-to-file]</p> <p>Becomes this yaml:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\ndata:\n    enemies=aliens\n    lives=3\n    enemies.cheat=true\n    enemies.cheat.level=noGoodRotten\n</code></pre> <p>OR</p> <p>Can also load it straight via kubectl with the <code>--from-literal=thing</code> flag.</p>"},{"location":"practices/containerization/kubernetes/configmaps/#using-a-configmap","title":"Using A ConfigMap","text":"<p>kubectl get cm [name] -o yaml</p> <p>Can load as an env via yaml that looks like this:</p> <pre><code>apiVersion: v1\n...\nspec:\n    template:\n    ...\n    containers:\n    env:\n    - name: ENEMIES\n      valueFrom:\n        configMapKeyRef:\n            name: app-settings\n            key: enemies\n</code></pre> <p>OR</p> <pre><code>apiVersion: v1\n...\nspec:\n    template:\n    ...\n    containers:\n        envFrom:\n        - configMapRef:\n          name: app-settings\n</code></pre> <p>OR</p> <p>\"VOLUME\"</p> <pre><code>apiVersion: v1\n...\nspec:\n    template:\n    ...\n    volumes:\n        - name: app-config-vol\n          configMap:\n            name: app-settings\n    containers:\n        volumeMounts:\n            - name: app-config-vol\n              mountPath: /etc/config\n</code></pre>"},{"location":"practices/containerization/kubernetes/configmaps/#uses","title":"uses","text":"<p>Displaying a value from a configMap all the way over into the front-end via a Docker image which is being served in a cluster. </p>"},{"location":"practices/containerization/kubernetes/configmaps/#viewing","title":"viewing","text":"<p>kubectl get cm -name-of-cm -o</p>"},{"location":"practices/containerization/kubernetes/deployments/","title":"Deployments","text":"<p>A ReplicaSet is a declarative way to manage pods. A deployment is even higher than that, it's a declarative way to manage pods using a ReplicaSet. </p> <p>Deployments (via ReplicaSets) take care of ensuring pods stay running (or get replaced). A ReplicaSet sits outside a pod. it's responsible for healing (which is really a way to say resurrecting). Provides fault-tolerance, scaling, ensuring requested number of pods are available. ReplicaSets are used by deployments.</p> <p>So Deployments utilize ReplicaSets to create Pods which contain Containers. Deployments give us zero-downtime updates, as well as rollbacks, autoscaling, and create unique labels.</p> <pre><code>apiVersion: v1\nkind: Deployment\nmetadata:\n    name: frontend\n    labels:\n        app: my-nginx\n        tier: frontend\nspec:\n    selector:\n        matchLabels:\n            tier: frontend\n    template:\n        metadata:\n            labels:\n                tier: frontend\n        spec:\n            containers:\n                - name: my-nginx\n                  image: nginx:alpine\n</code></pre> <p>To actually run it:  kubectl apply -f file.deployment.yml (or create, but usually apply)</p> <p>kubectl get deployments kubectl get deployment --show-labels - used to organize things kubectl get deployment -l app-nginx - find just the things with that label kubectl delete deployment [deployment-name]</p> <p>these two do the same thing mostly  kubectl scale deployment [deployment-name] --replicas=5 kubectl scale deployment -f file.deployment.yml  --replicas=5</p> <p>Can also add replicas right in the yml file itself.</p>"},{"location":"practices/containerization/kubernetes/deployments/#constraints","title":"Constraints","text":"<p>you can limit the resources used by a deployment in the spec. </p> <p>kubectl descrieb deployment [name-of-deployment] kubectl describe deployments kubectl get deployments -l app=my-nginx kubectl scale -f filename.yml --replicas=4</p> <p>Zero downtime deployments allow software updates to be deployed to production without impacting end users. This is a huge benefit of K8s.</p> <p>Options: - rolling - kind of like the default - blue/green - split between old and new - canary - a small amount of traffic goes to one pod - rollbacks - oops</p>"},{"location":"practices/containerization/kubernetes/deployments/#rolling","title":"Rolling","text":"<p>New pods more or less replace older pods one at a time until only new pods are left. This is the automatic method if you do <code>kubectl apply -f file.deployment.yml</code>. </p>"},{"location":"practices/containerization/kubernetes/kubectl/","title":"Kubectl","text":"<p>create -f filename.yml --save-config describe pod [pod-name] apply -f filename.yml kubectl describe - name kubectl exec -it sh - you're inside the pod via shell kubectl edit -f filename.yml - live editor in the cli</p>"},{"location":"practices/containerization/kubernetes/kubernetes/","title":"Kubernetes","text":"<p>Docker compose (vs dockerfile):</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#kubernetes","title":"Kubernetes","text":"<p>System for automating deployment, scalping and management of containerized applications It\u2019s like docker compose, but with scaling and health and everything else Like a conductor of a container orchestra, so to speak Load balancing, service discovering Storage orchestration Automate rollouts/rollbacks Self-healing Secret and config management Horizontally scale</p> <p>Moves from current to desired state (adding containers, what have you)</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#pod","title":"Pod","text":"<p>The packaging/box for a container. A container sits in a pod (Spacesuit, with a container as the person inside)</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#deploy-pods","title":"Deploy pods","text":""},{"location":"practices/containerization/kubernetes/kubernetes/#services","title":"Services","text":"<p>Resources you can use to check on pods</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#nodes","title":"Nodes","text":"<p>Master node has an etcd store, which has everything needed to track what\u2019s happening inside it Also a controller manager and a scheduler Also an api server Kubectl connects to the master Master communicates with the various nodes</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#kubelet","title":"Kubelet","text":"<p>Software running on the node that allows communication with the master</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#why-kubernetes","title":"Why Kubernetes","text":"<p>Assuming containers: accelerate dev onboarding, columns, eliminate app conflicts, environment consistency</p> <p>Orchestrate containers, zero-downtime deployments, self-healing powers, scale containers \u2026 and more</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#why-kubernetes-for-a-developer","title":"Why Kubernetes for a developer","text":"<p>Emulate production locally. Move from Docker Compose to K8s. Create an e2e testing env. Ensure app scales properly. Ensure secrets/config are working properly. Performance testing. Workload scenarios (ci/cd, etc). Learn how to leverage deployment options. Help devops cerate resources and solve problems.</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#options-for-running-k8s-locally","title":"Options for running K8s locally","text":"<p>Minikube Docker desktop</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#run-in-docker-desktop","title":"Run in docker desktop","text":"<p>Simple as enabling k8s inside docker desktop. Windows or Mac OS. </p>"},{"location":"practices/containerization/kubernetes/kubernetes/#using-kubectl","title":"Using kubectl","text":"<ul> <li>kubectl version </li> <li>kubecl cluster-info</li> <li>kubectl get all kubectl run [container-name] --image=[image-name] kubectl port-forward [pod][ports] kubectl expose ... bubectl create [resource] bubectl apply [resource]</li> </ul> <p>alias k=\"kubectl\"</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#k8s-web-ui-dashboard","title":"K8s Web UI Dashboard","text":"<p>Enable: - kubectl apply [dashboard-yaml-url] - kubectl apply -f dashboard.adminuser.yml - kubectl create token admin-user -n kube-system - kubectl proxy - visit the dashboard url and login</p>"},{"location":"practices/containerization/kubernetes/kubernetes/#links","title":"Links","text":"<ul> <li>https://github.com/kubernetes/examples # all kinds of official examples of how to do things - a source of truth</li> <li>https://www.pluralsight.com/courses/kubernetes-developers-core-concepts # many of these docs came from watching this course</li> <li>https://github.com/DanWahlin/DockerAndKubernetesCourseCode # GitHub repo with all the examples and such for this course</li> </ul>"},{"location":"practices/containerization/kubernetes/pods/","title":"Pods","text":"<p>The basic execute unit of a K8s application. Pods run containers, one or theoretically more than one. Think of pods as the way to organize parts into pods - normally a single process per container, and a single container per pod.</p> <p>Has: ip address memory volumens etc.</p> <p>Pods live and die but never come back to life</p> <p>Master scheduled one or more pods in a node. Pods will have a unique Ip address. Pod containers shar ethe same network namespace. Container processes need t bind to different ports within a pod. (A sidecar container would need its own port if it's at the same internal IP address.)</p> <p>Pods NEVER span nodes.</p>"},{"location":"practices/containerization/kubernetes/pods/#creating-pods","title":"Creating Pods","text":"<p>kubectl run command is an easy, but non-declarative way (imperative) to do it. Kubectl create/apply works with yaml files. </p> <p>Ports are only acessible within a cluster by default. you can expose ports externally via kubectl port-forward <code>kubectl port-forward [name-of-pod] 8080:80</code> where 8080 is extenral and 80 is internal. This is the most basic way to accomlish this.</p>"},{"location":"practices/containerization/kubernetes/pods/#deleting-pods","title":"Deleting Pods","text":"<p>The big idea here is that you can delete a pod but if you don't delete the deployment it'll just come back again.</p>"},{"location":"practices/containerization/kubernetes/pods/#common-commands","title":"Common commands","text":"<ul> <li>kubectl run my-nginx --image=nginx:alpine (THIS IS JUST THE NAME OF A DOCKER IMAGE - SO IT'S LOOKING AT DOCKER'S REGISTRY) </li> <li>kubectl delete </li> <li>kubectl get all </li> <li>kubectl port-forward my-nginx 8080:80</li> <li>kubectl delete pod [pod-name]</li> </ul>"},{"location":"practices/containerization/kubernetes/pods/#health","title":"Health","text":"<p>Probes are diagnistic performed by the kubelet on a container. </p> <p>Liveness: is the pod sick or healthy? Readiness: is it ready to receive requests?</p> <p>Example of a liveness probe:</p> <pre><code>apiVersion: v1\nkind: Pod\n...\nspec:\n    containers:\n    - name: my-nginx\n      image: nginx:alpine\n      livenessProbe:\n        httpGet:\n            path: /index.html\n            port: 80\n        initialDelaySeconds: 15\n        timeoutSeconds: 2\n        periodSeconds: 5\n        failureThreshold: 1\n</code></pre>"},{"location":"practices/containerization/kubernetes/rollouts/","title":"Rollouts","text":"<p>Since the term deployment is already used to apply to something that is smaller than a complete release of all the assets of your application, I'm using the term rollout to refer to an orchestrated release of multiple resources (potentially including deployments, but also secrets and configmaps and everything else).</p> <p>Steps might be:</p> <ul> <li>create secrets using kubectl</li> <li>kubectl apply -f .k8s # where k8s contains a number of yml files of all flavors</li> </ul>"},{"location":"practices/containerization/kubernetes/secrets/","title":"Secrets","text":"<p>An object that contains a small amount of sensitive data.</p> <p>Kubernetes has away of storing sensitive info. Can mount as env variables or files. Only makes secrets available to nodes that have a pod requesting the secret. And secrets are stored in tmpfs on a node (not on disk).</p> <p>Definitely want to limit access to etcd where secrets are stored. RBAC should be used to create pods because that's a way to back into secrets.</p>"},{"location":"practices/containerization/kubernetes/secrets/#creating","title":"Creating","text":"<p>kubectl create secret generic my-secret --from-literal=pwd=my-password kubectl create secret generic my-secret --from-file=ssh-privatekey=~/.ssh/rsa_pub</p> <p>Via yaml? Yes, but it's only base64 encoded!</p> <pre><code>kind: Secret\nmetadata:\n    name: db-passwords\ntype: Opaque\ndata:\n    app-password: Base64etc=\n    admin-password: Base64etc=\n</code></pre>"},{"location":"practices/containerization/kubernetes/secrets/#accessing","title":"Accessing","text":"<p>kubectl get secrets kubectl get secrets db-passwords -o yaml</p> <pre><code>apiVersion: v1\n...\nspec:\n    template:\n    ...\n    containers:\n    env:\n    - name: DATABASE_PASSWORD\n      valueFrom:\n        configMapKeyRef:\n            name: db-password\n            key: db-password\n</code></pre> <p>... and so on, in the same way you get values out of configmaps.</p>"},{"location":"practices/containerization/kubernetes/services/","title":"Services","text":"<p>Services talk inside the pod, and from outside the cluster.</p> <p>A single point of entry to access one o rmore pod. Simple example: if an IP changes, a service tracks that change. </p> <ul> <li>Abstract pod IP addresses from consumers</li> <li>Load balance between pods</li> <li>Relies on labels to associate a service with a pod</li> <li>Node's kube-proxy creates a virtual IP for services</li> <li>Utilizes layer 4 (TCP/UDP over IP)</li> <li>Creates endpoints that sit between the sercice and the pods</li> </ul> <p>So you might have a front-end service that talks to ssveral load balanced pods, and then a backend service that only talks to one.</p> <p>It appears that services actually RUN on the NODE itself, rather than on a pod (or as a separate pod).</p>"},{"location":"practices/containerization/kubernetes/services/#types","title":"Types","text":"<p>ClusterIP Expose the service on a clusetr-internal IP (default) The service itself is only used withint the cluster. Pods can talk to other pods via this ClusterIP service.</p> <p>NodePort Expose the sercice on each ode's IP at a static port. This allocates a port frome a range by default. Each node proxies the allocated port. So this allows an external caller to proxy into a pod (or pods). Useful for debugging or somethign else.</p> <p>LoadBalancer Provision an external IP to act as a load balancer for the service. Exposes a service externally. Particularly useful when combined with a cloud providers load balancer.</p> <p>ExternalName Maps a service to the DNS name The service just acts as an alias for an external service. So the service just proxies over to the external service. It's a sync from outside into inside, and the external service details are hidden from the cluster.</p>"},{"location":"practices/containerization/kubernetes/services/#kubectl","title":"kubectl","text":"<p>Using kubectl to access the insdie of a cluster via a service.</p> <p>kubectl port-forward pod/pod-name 8080:80 kubectl port-forward deploymeny/deploymnet-name 8080:80 kubectl port-forward service/service-name 8080:80</p>"},{"location":"practices/containerization/kubernetes/services/#with-yaml","title":"With YAML","text":"<p>A simple example.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: frontend # each service automatically gets an ip address - so there will be something available at frontend:80\n    labels:\n        app: my-nginx\nspec:\n    type: ClusterIP # this is the default - can be NodePort, LoadBalancer, or ExternalName with different sub-options\n    selector:\n        app: nginx\n    ports:\n        name: http\n        port: 80\n        targetPort: 80\n</code></pre>"},{"location":"practices/containerization/kubernetes/services/#deploy-yaml-with-kubectl","title":"Deploy yaml with kubectl","text":"<ul> <li>kubectl create -f file.service.yml</li> <li>kubectl apply -f file.service.yml </li> </ul> <p>As always create/apply do a lot of the same thing, and there are valid reasons to use either/both.</p>"},{"location":"practices/containerization/kubernetes/services/#test-to-see-if-a-service","title":"Test to see if a service","text":"<p>Sneak right into a pod, then see if that pod can talk to other pods.</p> <ul> <li>kubectl exec (pod-name) -- curl -s http://podIP</li> </ul>"},{"location":"practices/containerization/kubernetes/services/#in-action","title":"In action","text":"<p>Get a pod's IP</p> <p>kubectl get pod name-of-pod -o # -o means yml</p> <p>Copy ip address. Then curl from point a to point b.</p> <p>OR</p> <p>Use the service's IP itself. So from a pod, you can hit the service's IP and it should take you over to the correct pod. You can also use the internal dns name like my-nginx:8080.</p> <p>Slightly different techniques for each type of service, but the same idea is relevant.</p>"},{"location":"practices/containerization/kubernetes/storage/","title":"Storage","text":"<p>If a container/pod goes down, how do you keep your data?</p> <p>A storage volume exists \"alongside\" your pods.</p>"},{"location":"practices/containerization/kubernetes/storage/#volumes","title":"Volumes","text":"<p>Not the only option, but it answers the question how do you store data and state for pods and containters, and also for sharing purposes. Logs and data for a database, the containers rely on a mountPath to access the volume. Each pod can have one or more volume attached.</p> <p>A volue refernec ea a storage location. Must have a unique name. Attached to a pod and may or not be tied to the lifetime.</p> <p>They generally get created declaratively via yaml, so there aren't any kubectl examples here so far.</p> <p>An example of creating an emptyDir type of volume in a simple pod. It launches an alpine pod that updates the index.html at the shared location with the current time every 10 seconds. </p> <p>A yaml example:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n    name: my-pv\nspec:\n    capacity: 10Gi\n    accessModes:\n        - ReadWriteOnce\n        - ReadOnlyMany\n    persistentVolumeReclaimPolicy: Retain\n    azureFile:\n        secretName: &lt;azure-secret&gt;\n        shareName: &lt;name_from_azure&gt;\n        readOnly: false\n</code></pre> <p>And a corresponding PVC:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: pv-dd-account-hdd-5g\n    annotations:\n        volume.beta.kubernetes.io/storage-class: accounthdd\nspec:\n    accessModes:\n        - ReadWriteOnce\n    resources: \n        requests:\n            storage: 5Gi \n</code></pre> <p>And in a corresponding Pod:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: pod-uses-account-hdd-5g\n    labels:\n        name: storage\nspec:\n    containers:\n        - image: nginx\n          name: az-c-01\n          command:\n          - /bin/sh\n          - -c\n          - while true; do echo $(date) &gt;&gt; /mnt/blobdisk/outfile; sleep 1; done\nvolumeMounts:\n- name: blobdisk01\n  mountPath: /mnt/blobdisk\nvolumes:\n- name: blobdisk01\n  persistentVolumeClaim:\n    claimName: pv-dd-account-hdd-5g\n</code></pre> <p>kubectl describe pod [pod-name] will show any volumes, as will viewing the yaml.</p>"},{"location":"practices/containerization/kubernetes/storage/#emptydir","title":"emptyDir","text":"<p>mepty dir, it's transiet. Lives and dies along with the containing pod. </p>"},{"location":"practices/containerization/kubernetes/storage/#hostpath","title":"hostPath","text":"<p>pod mounts into the nodes filesystem. So for example, it might mount to the Docker socket. Pros: easy to set up. Cons: what if the worker goes down. There are a lot of options within this hostPath structure. So this means Docker is available (and shared, pretty much) within the pod that has this volume shared. </p>"},{"location":"practices/containerization/kubernetes/storage/#nfs","title":"nfs","text":"<p>mounts to a share, typically provided by a cloud service. Super handy for this kind of thing, because the storage gets all the magic of a cloud provider's storage solutions, but is still available via your cluster.</p>"},{"location":"practices/containerization/kubernetes/storage/#persistentvolumes","title":"PersistentVolumes","text":"<p>A cluster-wide storage unit provisioned by an administrator with a lifecycle independent from a pod. Maybe to a cloud storage solution, maybe to a NAS. It's abstracted away. PVCs (PersistentVolumeClaims) are how you connect to it.</p> <p>A yaml example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: my-nginx\nspec:\n    volumes:\n        - name: html\n          emptyDir: {} \n    containers:\n        - name: nginx\n          image: nginx:alpine\n          volumeMounts:\n            - name: html\n              mountPath: /usr/share/nginx/html # this is nginx's default serving path\n              readOnly: true\n        - name: html-updater\n          image: alpine\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - while true; do date &gt;&gt; /html/index.html; sleep 10; done\n          volumeMounts:\n            - name: html\n              mountPath: /html # doesn't matter so much where this is mounted since it's just doing an update\n</code></pre>"},{"location":"practices/containerization/kubernetes/storage/#persistentvolumeclaim","title":"persistentVolumeClaim","text":"<p>The connection flow, technically, goes like this: Pod &gt; PVC &gt; PV &gt; STORAGE</p>"},{"location":"practices/containerization/kubernetes/storage/#storageclasses","title":"StorageClasses","text":"<p>Type of storage template that can e used to dynamically provision storage. Used to define different \"classes\" of storage. Acts as a type of storage template. Supports dynamic provisioning of PersistentVolumes. Admins dont' have to create PVs ahead of time if it's set up this way.</p> <p>Create a PVC that references the SC, which sort of sits between the PVC and the PV - but the PVC connects directly back to the PVC. </p> <p>Yaml for a local storage PersistentVolume</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n    name: my-pv\nspec:\n    capacity: 10Gi\n    volumeMode: Block\n    accessModes:\n        - ReadWriteOnce\n    storageClassName: local-storage\n    local:\n        path: /data/storage\n    nodeAffinity:\n        required:\n            nodeSelectorTerm:\n            - matchExpressions:\n              - key: kubernetes.io/hostname\n                operator: In\n                values:\n                - &lt;node-name&gt;\n</code></pre> <p>and the corresponding claim</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: my-pvc\nspec:\n    accessModes:\n    - ReadWriteOnce\n    storageClassName: local-storage\n    resources: \n        requests:\n            storage: 1Gi \n</code></pre> <p>And the corresponding pod:</p> <pre><code>apiVersion: v1\nkind: Pod\n...\nspec:\n    volumes:\n    - name: my-volume\n      persistentVolumeClaim:\n        claimName: my-pvc\n</code></pre>"},{"location":"practices/containerization/kubernetes/troubeshooting/","title":"Troubleshooting","text":"<p>kubectl logs [pod-name] kubectl logs [pod-name] -c [container-name] kubectl logs -p [pod-name] kubectl logs -f [pod-name] kubectl describe [pod-name] kubectl describe [pod-name] -o yaml | json # defaults to yaml</p>"},{"location":"practices/containerization/kubernetes/yaml/","title":"K8s yml","text":"<p>this is a viable way, but isn't necessarily the default. Good to know because it's a good fundamental.</p> <p>This is identical to using kubectl to do this manually.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: my-nginx\nspec:\n    containers: \n    - name: my-nginx\n    image: nginx:alpine\n</code></pre> <p><code>kubectl create -f file.pod.yml --dry-run --validate=true</code> <code>kubectl create -f file.pod.yml</code> <code>kubectl apply -f file.pod.yml</code> - this works if it exists or if it doesn't; create will give you an error if it already exists, while apply will make updates. It also creates if it hasn't alreadty been created. <code>kubectl create -f -file.pod.yml --save-config</code> - use --save-config when you want to use kubectl apply in the future</p> <p>This talks to the master node and processes the request. Save config causes the reousr'ces confi settings to be saved in annotations. It actually saves it in the metadata of the yml. I think.</p> <p>You can delete then via the yml file that was used to create, just by using \"delete\" instead of \"create\".</p>"},{"location":"practices/source-control/git/","title":"Git","text":"<p>In Git, rebase is a command that integrates changes from one branch into another. It's an alternative to the merge command. The key difference between rebase and merge is that rebase rewrites the commit history in order to produce a straight, linear succession of commits.</p>"},{"location":"practices/source-control/recommended-gitlab-mr-process/","title":"Merge Request Process","text":"<p>A good merge request process - that everyone adheres to - is critical to a well-functioning software development team. </p>"},{"location":"practices/source-control/recommended-gitlab-mr-process/#gitflow","title":"Gitflow","text":"<p>Gitflow is a merge request process/branching strategy that has gained traction over several years. While other, less strict Git strategies have emerged since Gitflow initially came on the scene, Gitflow remains an excellent balance of organization, approval, and CI/CD integration.</p> <p>!!! note     There are many good visualizations of Gitflow on the internet, including from Atlassian. It's an important concept to have locked in, so take the time to research it, if necessary.</p>"},{"location":"practices/source-control/recommended-gitlab-mr-process/#how-it-works","title":"How It Works","text":"<p>The goal of Gitflow is to make sure commits make their way through from developer environment all the way to production, with a minimum of conflicts and risk of errors or substandard code. In so doing, several different branches are utilized to keep concerns separate.</p> <ul> <li><code>master</code> - The \"main\" branch. Releases are generated at a regular interval from <code>master</code>. <code>master</code> will, in general, only receive commits from <code>develop</code>.</li> <li>The exception is if there needs to be a hotfix or some other off-cycle release. Then a commit may be made from a feature branch into <code>master</code>, but it goes without saying that this is to be avoided whenever possible.</li> <li><code>develop</code> - The integration branch for features, bugs, and chores. <code>develop</code> receives commits from these individual feature branches. These commits are then collectively merged into <code>master</code>.</li> <li>Feature branches</li> <li><code>feature</code> - A branch that contains new functionality in one way or another supporting the end user experience. Named in association with the JIRA ticket, following the pattern of <code>feature/XXXX-1234 - Ticket description goes here</code>.</li> <li><code>bug</code> - A branch that addresses an error or failure that is impacting the end user experience. Named in association with the JIRA ticket, following the pattern of <code>bug/XXXX-1234 - Ticket description goes here</code>. When a bug is ready to be merged, it must be merged in to both <code>master</code> and <code>develop</code> to keep these branches in sync and avoid conflicts down the line.</li> <li><code>chore</code> - A branch that addresses tech debt, or some other maintenance task that does not actually impact the end user experience. Named in association with the JIRA ticket, following the pattern of <code>chore/XXXX-1234 - Ticket description goes here</code>.</li> </ul>"},{"location":"practices/source-control/recommended-gitlab-mr-process/#approvals","title":"Approvals","text":"<p>Use the Gitlab interface to create a merge request, making sure to follow the steps in the merge request template to verify the code is ready for <code>develop</code>. Then, select a project \"maintainer\" as a Reviewer, and select a peer as an Assignee. Both of these individuals must approve your code before it can be merged into <code>develop</code>.</p> <p>!!! note     Merge Requests can be stressful, but be confident! Firstly, be confident when you are giving feedback; you have good ideas and know what you're doing - otherwise, you wouldn't be in this position! If you see something that could be refactored or doesn't make sense, say something. Don't assume that it is correct, even if the person you're reviewing for is more senior than you. You are both more likely to come out of a Merge Request process as better developers if you say something, whereas you'll neither one benefit if you just rubber stamp everything.</p>"},{"location":"practices/source-control/recommended-gitlab-mr-process/#releases","title":"Releases","text":"<p>Releases are generated off <code>master</code>, at a cadence to be determined on a team-by-team basis.</p>"},{"location":"practices/source-control/recommended-release-process/","title":"Release Process","text":"<p>This document is meant to provide a starting point for a potential release cadence. For applications that consume (or are consumed) APIs, artifacts, or other dependencies, having a predictable, consistent release cadence is critically important. This discipline, combined with tooling that provides predictability and lowers developer friction, will create pathways to faster development with fewer mistakes, which will lead to more time and energy to take up a new hobby. (Or whatever.)</p>"},{"location":"practices/source-control/recommended-release-process/#branch-structure","title":"Branch Structure","text":"<p>We will be taking as assumed that we are using <code>git</code> (hosted on Gitlab) for this process.</p> <p>There are a few different branching models that are compatible with this release strategy, but the most important is that there is a specific branch for releases. This can be the main/default branch, or it can be a secondary branch off of that called <code>release</code>. We will be making an assumption of the latter process in this document.</p> <p>So, your branch structure might look something like this:</p> <p></p> <p>(Bugfixes, etc., omitted for clarity. Obviously.)</p> <p>So, there are a few ways to go from this very simple starting place. You can have a release triggered after every commit to our main branch (<code>develop</code> in this example), or you can manually trigger releases via some other mechanism (maybe building locally). Another option, however, would be to add an additional branch, called release. This allows for collecting <code>x</code> commits to the main branch, and deploying only those commits, perhaps after a testing cycle, or on a pre-determined cadence matching sprints.</p> <p></p> <p>In this second example, a determination was made to release following the check-in of <code>Feature-001</code>. <code>Feature-002</code> is reserved for the following release.</p>"},{"location":"practices/source-control/recommended-release-process/#release-process_1","title":"Release Process","text":"<p>So, based on this second structure, we can build one of a few different release processes, all of which are supported and documented by Gitlab. We'll pick a specific one for the purposes of this proposal, understanding that there are a variety of valid approaches.</p>"},{"location":"practices/source-control/recommended-release-process/#create-a-release","title":"Create a Release","text":"<p>One good way to specify a release is by using a <code>Git</code> tag. A \"tag\" is a marker within the repository that creates a commit. This can then be utilized as a way to reference the repository as it exists at a certain point in time. By creating tags off the Release branch, we can restrict full releases to our package repositories as only taking place when we intend, but have flexibility to include whatever we need to as a release. (This allows for certain, unavoidable situations when the main branch and the release branch must differ, perhaps due to production hotfixes.)</p> <p></p> <p>So, either by using the Gitlab UI or a local Git client, on the Release branch, create a tag that matches the version number you would like to commit to. We will utilize a Semantic Versioning based approach to versioning.</p> <p>!!! alert     This manual versioning is very much, if you will allow the term, an <code>alpha</code> product. We will rapidly explore automatic versioning number bumping systems that make sense within the individual repositories.</p>"},{"location":"practices/source-control/recommended-release-process/#release-a-release","title":"Release a Release","text":"<p>Once this release is created, the individual repository's <code>.gitlab-ci.yml</code> will extend the default Release templates and execute the packaging and uploading processes defined therein.</p> <p>!!! alert     These default Release templates are WIP - specific usage notes will be updated here once they're further along. </p>"},{"location":"practices/source-control/recommended-release-process/#release-notes","title":"Release Notes","text":"<p>Also in the release templates is an \"Update Release Notes\" process, which can also be leveraged with your own strategy to automatically pull in information and create documentation on what is included in the most recent release. This can include resolved Issues directly from Gitlab, and/or a listing of all the commits that are new since the last tag, or something else. Regardless, this build step will log and distribute the release notes to provide transparency and predictability.</p>"},{"location":"tools/development/control-tower/","title":"Control Tower","text":"<p>AWS Control Tower is a solution for setting up and governing a secure, multi-account AWS environment. </p>"},{"location":"tools/development/docker/","title":"Docker","text":"<p>Docker is the primary containerization software used today. While it is different from a Virtual Machine, in terms of how it shares (or doesn't share) resources with the host computer, it can be thought of as a computer-inside-a-computer. Docker images can be extremely lightweight (a very popular implementation, Alpine Linux, is only 5 megabytes), and as such, a relatively modest modern host computer can run many simultaneous Docker instances. Containerizing individual applications in Docker provides consistency and portability, and is a significant player in modern software development.</p> <p>A Docker container typically consists of all the files required to run a specific application, along with building it. The base image to run in the instance, along with all the other startup build tasks, are defined in a <code>Dockerfile</code>.</p>"},{"location":"tools/development/docker/#cicd","title":"CI/CD","text":"<p>By providing an easy to build, consistent platform, Docker enables CI/CD pipelines to quickly create new instances of their application for Unit Testing, Integration Testing, deployment, and other tasks.</p>"},{"location":"tools/development/docker/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes works very well with Docker containers - in fact, in our context, Kubernetes is, at its core, a fleet of many Docker containers. By putting our applications inside Docker containers, we enable rapid, consistent deployment into Kubernetes clusters, with assurance that the application will run in many pods the way we expect it to.</p>"},{"location":"tools/development/docker/#dev-containers-in-vscode","title":"Dev Containers in VSCode","text":"<p>See this section for more.</p>"},{"location":"tools/development/gitlab-agent/","title":"Gitlab Agent","text":"<p>The Gitlab Agent, briefly put, provides a secure tunnel between our Gitlab installation and an EKS cluster, allowing for deployment of both internal and externally facing applications. Potential use cases include:</p> <ul> <li>Load-balanced, scalable backend microservices deployment in all environments</li> <li>Rapid builds, testing, and destroying CI/CD steps</li> <li>Maintenance and visibility into cluster health and security with the Agent configuration repository</li> </ul> <p>Once configured, the Agent \"watches\" the manifest files of projects within Gitlab (as specified in the Agent's <code>config.yaml</code> file), and then executes the instructions in those manifest files, which will include deployment instructions. It is designed to deploy within the same cluster, although build and test tasks can also be defined in these manifests. In this way, there is some overlap between what the Gitlab Agent and the Gitlab Runner can do, and keeping the use cases of the two straight can be a challenge.</p> <p>!!! note</p> <pre><code>Keep this in mind: a repository's `.gitlab-ci.yml` file utilizes a Gitlab Runner, where as the Gitlab Agent looks for Kubernetes manifests in the target repository. Confusingly, you can provision a Gitlab Runner via the Gitlab Agent, but not vice versa - a standalone Gitlab Runner cannot do the same things a Gitlab Agent can do.\n</code></pre>"},{"location":"tools/development/gitlab-agent/#the-gitlab-agent-configuration-repository","title":"The Gitlab Agent Configuration Repository","text":"<p>A specific repository in Gitlab provides the special use of administration for both the Agent(s) within our instance of Gitlab, but also the ability to quickly and easily install additional applications to verify the security and health of our EKS cluster. Find more information in Gitlab's documentation.</p>"},{"location":"tools/development/gitlab-agent/#registering-new-agents-in-gitlab","title":"Registering New Agents in Gitlab","text":"<p>You can register new agents in the cluster by adding a <code>config.yaml</code> file that matches the correct format in <code>./.gitlab/agents/name-of-agent/config.yaml</code> in the Gitlab Agent Configuration repository. It is not necessary to create new agents for each project or group, but there may be valid reasons to separate tasks between agents.</p> <pre><code>- .gitlab\n  - agents\n    - dev-agent\n    - prod-agent\n</code></pre>"},{"location":"tools/development/gitlab-agent/#example-configyaml","title":"Example <code>config.yaml</code>","text":"<p>An absolute barebones <code>config.yaml</code> would include the following information. This defines a GitOps section, defines the repositories to watch, a namespace to run the task in, and the paths to look for manifest files in.</p> <pre><code>gitops:\n  manifest_projects:\n    - id: \"example/path/to/config/\"\n      default_namespace: dev-agent-002\n      paths:\n        - glob: \"/**/*.yaml\"\n</code></pre>"},{"location":"tools/development/gitlab-agent/#example-manifest-file","title":"Example Manifest File","text":"<p>This manifest file will go somewhere matching the <code>glob</code> pattern in the above <code>config.yaml</code> file, in the repository that matches the path in the <code>gitops.manifest_projects.id</code> section. This file does not need to be called <code>manifest.yaml</code>, but it DOES need to end in <code>.yaml</code> for the glob to match.</p> <p>You can have one or several manifest files within the same project, and selectively execute them depending on your specifications.</p> <p>!!! note</p> <pre><code>If you have a manifest file in your repository, you can also have a `.gitlab-ci.yml`, or not. There is some overlap in terms of what they can do, but `.gitlab-ci.yml` has more visibility built in to the Gitlab GUI, but lacks the ability to securely connect with the Kubernetes cluster for deployment purposes.\n</code></pre> <p>A manifest is a Kubernetes term for a set of instructions for actions/deployments within a Kubernetes Cluster. A trivial manifest file is shown below:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: gitlab-gitops-test\n  namespace: default\ndata:\n  key: It works!\n\n</code></pre> <p>(This manifest won't actually do anything besides apply a new ConfigMap to the cluster. More complex examples coming soon.)</p>"},{"location":"tools/development/gitlab-agent/#create-a-gitlab-agent","title":"Create a Gitlab Agent","text":"<p>This article will guide you through creating and registering an instance of in a Kubernetes Cluster.</p>"},{"location":"tools/development/gitlab-agent/#creating-the-agent-in-gitlab","title":"Creating the Agent in Gitlab","text":"<p>The Gitlab Agent works by connecting your Gitlab installation with your Kubernetes Cluster. In order to do so, your Gitlab instance must be running something called KAS, or Kubernetes Agent Server. This is provided by default in Gitlab cloud hosted installations. The address of KAS for cloud hosted installations of Gitlab is <code>wss://kas.gitlab.com</code>. The cluster, meanwhile, needs to have software called <code>agentk</code> running in one or more pods.</p> <p>After creating the new <code>config.yaml</code>, you can then let Gitlab know about the agent and receive an authorization token.</p> <ul> <li>In the Agent Configuration repository, click Infrastructure &gt; Kubernetes clusters</li> <li>In the bottom right-hand corner of the agents box, click <code>Install a new agent</code></li> </ul> <p></p> <ul> <li>In the modal window that comes up, select your new agent out of the dropdown</li> <li>On the next screen, copy the token and store it somewhere temporarily</li> </ul> <p>!!! warning</p> <pre><code>This token is only available when you first visit this screen. There is no other way to acquire it, and if it is misplaced you will have to start over.\n</code></pre>"},{"location":"tools/development/gitlab-agent/#installingregistering-the-agent-in-the-kubernetes-cluster","title":"Installing/Registering the Agent in the Kubernetes Cluster","text":"<p>This example assumes Pulumi is being used to handle all of the cloud integrations (where possible) and this includes adding a new Gitlab Agent.</p> <p>In order to add a new agent:</p> <ul> <li>Clone the [epository that provisions the clusters and delivers software into the cluster</li> <li>Add a new file in <code>lib/clusters</code> similar to <code>gitlab_agent.py</code>, with the name of the file being the name of the agent (e.g. <code>gitlab_prod_agent.py</code>)</li> <li>Add a reference to this agent in <code>eks.py</code>, similar to   <code>gitlab_prod_agent.compose_gitlab_agent(         cluster_provider,         cluster,         account_info[\"gitlab_prod_agent_token\"],         account_name,         )</code>     where <code>gitlab_prod_agent</code> is the file being imported.</li> <li>Encrypt the token you acquired from Gitlab in the previous step</li> <li>Assuming this agent is going to sit in the same cluster as the existing Gitlab agent(s), that encrypted token will sit alongside <code>agent_token</code> in the <code>Pulumi.administration.yaml</code> file, perhaps called <code>gitlab_prod_agent_token</code> - note this token is retrieved from the <code>account_info</code> object on the 4th line of the snippet above</li> <li>If you copied <code>gitlab_agent.py</code>, make sure to update the <code>namespace_name</code> and <code>agent_name</code> variables near the top of the file, or else the installation will fail</li> </ul> <p>Now you're ready to register the new agent in the cluster. This Pulumi program will register a pod with <code>agentk</code> running on it, and then connect it to the Gitlab.</p>"},{"location":"tools/development/gitlab/","title":"Gitlab","text":"<p>Gitlab is a richly featured platform with a wide variety of modern software development and release features. It has broad capabilities, including:</p> <ul> <li>GitOps</li> <li>Source Control</li> <li>Kubernetes Cluster Administration</li> </ul> <p>It is critical that each team member have familiarity with the way Gitlab works, as every software operation flows through it. Gitlab runs on their own 'hosted' servers, or locally on a company's own servers. </p>"},{"location":"tools/development/gitlab/#git-101","title":"Git 101","text":"<p>Git is an open-source version control system. It is fast, flexible, and lightweight, as a result, is used very widely. A full implementation of Git requires a remote and a local instance. All of its local functionality is available via its CLI, but there a are a few commonly used UI helpers to aid in visualization of common tasks.</p>"},{"location":"tools/development/gitlab/#how-git-works","title":"How Git Works","text":"<p>As a version control system, Git's main job is to keep track of changes to a set of files. At it's most basic, this means that if Bob changes a file, and then Alice changes the same line of the same file, if Git is being used to track those changes, it will alert Alice to this fact and (hopefully) prevent Bob's code from being overwritten, or both changes being present, causing a failure.</p> <p>At it's core is a branching structure, which can be visualized as seen below, with <code>main</code> being the primarily branch and <code>feature</code> being a branch with a change that is being worked on by an individual developer. When taking on this work, the developer will start with <code>main</code>, and then create a branch called feature. They will do their work on their local computer, and when completed, they will use Gitlab to demonstrate their changes (using what is called in Gitlab a Merge Request, or MR).</p> <pre><code>     /------feature-----\\\n    /                    \\\n-------------main----------------\n</code></pre> <p>Git tracks the changes, sorts out the order in which they were made, and provides feedback to the developer when they are committing changes. Gitlab provides visibility into these functions while providing easy-to-use workflows. Other functionality provided in Gitlab extends out of this core, Git-based feature set.</p>"},{"location":"tools/development/gitlab/#git-tools","title":"Git Tools","text":"<p>As mentioned before, Git consists of at least two \"ends\" - a local and a remote. Each local instance will need some copy of Git's software in order to interact with the remote. Every VSCode Dev Container will come with Git installed in the CLI.</p> <p>In addition to using the command line inside VSCode, we recommends using the GitLens extension that is available inside VSCode, and will typically be installed inside the Dev Container as well.</p>"},{"location":"tools/development/gitlab/#project-structure","title":"Project Structure","text":""},{"location":"tools/development/gitlab/#gitops","title":"GitOps","text":""},{"location":"tools/development/kubernetes/","title":"Kubernetes","text":"<p>Kubernetes is a container orchestration platform. In very simple terms, this allows developers to control sets of individual instances of a container to accomplish deployments, scale (and descale) rapidly, and other tasks involving large sets of machines.</p>"},{"location":"tools/development/kubernetes/#terminology","title":"Terminology","text":"<p>A few terms and technologies, briefly explained.</p> <ul> <li>POD - The smallest unit of execution. A pod is an individual \"machine\", predominantly a Docker container. It has a unique IP address. Can be anything from a whole LAMP stack to a purpose-built application.</li> <li>SERVICE - something that routes traffic to a logical set of pods.</li> <li>VOLUMES - Storage associated with a pod. Can be Persistent, but by default it is ephemeral.</li> <li>NAMESPACES - Used to organize application clusters within the same logical group.</li> <li>CONTROLLERS - Manage the state of the cluster. Exists on the control plane.</li> <li>CONTROL PLANE - One or more individual pods that control the activity of the other pods.</li> <li>KUBECTL - The Kubernetes command line tool, used for a variety of functionality. The primary way to interface with a Kubernetes instance.</li> <li>KUBECONFIG - A file that defines the Kubernetes cluster(s) that you wish to control from your local machine. Located by default at <code>./root/.kube/config</code>.</li> <li>HELM - A command line tool for software installation of Kubernetes installations, think package manager.</li> <li>EKS - Amazon Web Service's purpose-built Kubernetes hosting service - stands for Elastic Kubernetes Service.</li> </ul>"},{"location":"tools/development/kubernetes/#advanced-terminology","title":"Advanced Terminology","text":"<p>More advanced concepts, not necessarily exclusive to Kubernetes.</p> <ul> <li>RBAC - Role-Based Access Control. This is a methodology for controlling access by assigning users to groups, and giving those groups permissions.</li> <li>Service Account - In Kubernetes, a service account is a special object that allows assignment of a Kubernetes RBAC role to a specific pod. Each namespace receives a service account by default.</li> <li>OIDC - OpenID Connect. An authentication protocol supported within Kubernetes clusters.</li> <li>Kubernetes RoleBindings - A way to assign a user to an RBAC group within Kubernetes. A RoleBinding is constrained to a specific namespace.</li> <li>Kubernetes ClusterRoleBindings - A way to assign a user to an RBAC group within Kubernetes. A ClusterRoleBinding is scoped to the entire cluster, rather than a specific namespace.  !!! note     RoleBindings and ClusterRoleBindings \"are similar to IAM Roles in that they define a set of actions (verbs) that can be performed against a collection of Kubernetes resources (objects).\" Source</li> <li>IRSA - IAM Roles for Service Accounts. Rather than simply using the default service account, IRSA creates a role assumption within the cluster, validates a provided token and then assigns a temporary AWS role credential.</li> </ul>"},{"location":"tools/development/postman/","title":"Postman","text":"<p>Postman is the industry standard API testing platform. It provides a simple interface to send requests to API endpoints with many configuration options. More sophisticated implementations include the ability to conduct automated API tests, share suites of tests and configuration, and remote saving of batteries of tests.</p>"},{"location":"tools/development/pulumi/","title":"Pulumi","text":"<p>The <code>__main__.py</code> file is the core of the Pulumi application.</p> <p>If you are unsure about what Pulumi does, you will (obviously) be best off spending some time with their documentation. But in (very) brief, Pulumi allows a developer to create and deploy cloud resources using code, and provides libraries for multiple languages. It differs from Terraform mainly in its flexibility, verbosity, and similarity to the rest of the environment a developer works in, allowing greater homogeneity of environments and therefore a smaller learning curve and integration with existing codebases. It also provides tremendous value by exposing tracking, visibility, and statuses via <code>pulumi.com</code>, which acts as a sort of Dashboard for all of the organization's infrastructure-as-code deployments.</p> <p>This doc will assume the usage of the <code>Python</code> flavor of Pulumi. Every individual infrastructure project that utilizes Pulumi will operate in a similar fashion.</p>"},{"location":"tools/development/pulumi/#provision-the-python-venv-and-install-required-modules","title":"Provision the Python <code>venv</code> and Install Required Modules","text":"<p>In order to develop using Pulumi, follow these steps:</p> <ul> <li>Open a new command prompt using VSCode's Terminal menu.</li> <li>Run the below commands to prepare the Python virtual environment Pulumi will be using. Make sure to update the <code>&lt;name-of-repository&gt;</code>.</li> <li><code>$ python3 -m venv /workspaces/&lt;name-of-repository&gt;/venv</code></li> <li><code>$ /workspaces/&lt;name-of-repository&gt;/venv/bin/python -m pip install -r requirements.txt</code></li> <li>Your development environment is ready!</li> </ul>"},{"location":"tools/development/pulumi/#pulumi-basics","title":"Pulumi Basics","text":"<p>The basic steps and common commands when working with Pulumi:</p> <ul> <li><code>pulumi login</code> will prompt you for your <code>pulumi.com</code> credentials</li> <li><code>pulumi stack init name-of-stack</code> will initialize a local instance of a 'stack'</li> <li><code>pulumi config set aws:region x-xxxx-x</code> will set a local config file with the appropriate aws region</li> <li><code>pulumi import xxxxxx</code> will reach out to an existing resource in AWS and provide a copy/pasteable snippet to enter into your code</li> <li><code>pulumi preview</code> will run through the steps included in your Pulumi code and display the list of tasks to be undertaken</li> <li><code>pulumi up</code> will execute the code and attempt to deploy real resources in the AWS account associated with the credentials configured</li> <li><code>pulumi stack output</code> will display the output configured at <code>pulumi.export</code></li> </ul> <p>When you run <code>preview</code> and <code>up</code>, what actually happens is that Pulumi creates local virtual environment - in our case in the 'venv' directory - and uses that to actually run the deployment (or mock deployment) tasks. This creates a lot of files in a directory called (in our case) venv. This whole directory should be in the .gitignore, as it's just temporary.</p> <p>If at some point you see the following message in the bottom-right-hand corner of VSCode, feel free to accept it: \"We've noticed a new workspace folder has been detected. would you like to select it for the container?\"</p>"},{"location":"tools/development/pulumi/#reset-your-pulumi-instance","title":"Reset your Pulumi instance","text":"<p>Sometimes you will deploy real resources to AWS and need to reset and try again. The following steps may help to reset your environment.</p> <ul> <li>Remove stack   <code>pulumi stack rm name-of-stack --force</code></li> <li>Delete your assets in AWS that were created using the AWS UI</li> <li>Re-add stack   <code>pulumi stack init name-of-stack</code></li> <li>Reset region   <code>pulumi config set aws:region us-east-2</code></li> <li>Make your adjustments to the codebase and save</li> <li>Do <code>pulumi preview</code> to see if everything looks okay</li> <li>Do <code>pulumi up</code> to provision/update resources</li> </ul>"},{"location":"tools/development/pulumi/#helpful-pulumi-commands","title":"Helpful Pulumi Commands","text":"<p>Here are a collection of handy commands when you are trying to troubleshoot a problem you're seeing with Pulumi.</p>"},{"location":"tools/development/pulumi/#verbose-logging","title":"Verbose Logging","text":"<p>Select an number 1 through 9 to determine the level of log level.</p> <ul> <li><code>pulumi up --verbose &lt;int 1-9&gt; --logtostderr</code></li> </ul>"},{"location":"tools/development/pulumi/#getting-the-urn-of-resources","title":"Getting the URN of Resources","text":"<ul> <li><code>pulumi stack -u</code></li> </ul>"},{"location":"tools/development/pulumi/#targeting-a-specific-resource-ignoring-others","title":"Targeting a Specific Resource, Ignoring Others","text":"<p>Just one resource:</p> <ul> <li><code>pulumi up --target 'urn-of-resource-being-targeted'</code></li> </ul> <p>Example, with removal of all dependents:</p> <ul> <li><code>pulumi destroy --target \"urn:pulumi:administration::core-tooling::eks:index:Cluster::eks-cluster\" --target-dependents</code></li> </ul>"},{"location":"tools/development/pulumi/#refreshing-from-the-remote-stack-to-current","title":"Refreshing \"From\" the Remote Stack to Current","text":"<ul> <li><code>pulumi refresh --target 'urn:pulumi:administration::core-tooling::gitlab:index/group:Group::name-of-group'</code></li> </ul>"},{"location":"tools/development/pulumi/#logging-messages-to-the-console","title":"Logging Messages to the Console","text":"<p>Pulumi's log module contains several logging methods: info, debug, warn, and error.</p> <pre><code>import pulumi\n\nprint_me = \"print me!\"\n\npulumi.log.info(\"print me\", print_me)\n</code></pre>"},{"location":"tools/development/vscode/","title":"VSCode","text":"<p>VSCode is developed my Microsoft and is valued for it's relatively light weight, speed, and tremendous flexibility and extensibility.</p>"},{"location":"tools/development/vscode/#dev-container","title":"Dev Container","text":"<p>One of the most powerful extensions in VSCode is the Dev Container (also called a Remote Container). A VSCode Dev Container is an instance of VSCode that can be opened and run inside an individual Docker container. The contents and configuration of this container is defined by files in the <code>.devcontainer</code> directory that ships with the repository. The <code>devcontainer.json</code> file is specific to VSCode, and defines a wide variety of options for the Dev Container. There is also the option of a standard <code>Dockerfile</code> which the <code>devcontainer.json</code> defines.</p> <p>Most (if not all) of the Dev Containers in use start with an extremely lightweight Linux instance, either the well-known Alpine distribution, or another distribution that starts with Alpine but adds additional software. Then, the <code>Dockerfile</code> and <code>devcontainer.json</code> files will install additional software and configuration. If something goes amiss, you can always reset a Dev Container. (There are several options available when using a running container available by clicking on the blue (depending on your color theme) Dev Container button in the bottom left-hand corner of VSCode.)</p>"},{"location":"tools/development/vscode/#running-a-dev-container","title":"Running a Dev Container","text":"<p>Running a Dev Container is typically just a matter of seeing notification that appears in the bottom right-hand corner of VSCode when you open a repository that features the definition files for container. If you miss that notification, the blue icon on the bottom right-hand corner should offer the option of reopening in a container.</p>"},{"location":"tools/development/vscode/#rebuilding-a-dev-container","title":"Rebuilding a Dev Container","text":"<p>If something goes wrong with your project, a good option is to rebuild your Dev Container. While this doesn't clear the cache (more on that in a moment), it will totally destroy and rebuild a new Docker container, which will often clear up issues. Either click on the Dev Container menu in the bottom left-hand corner and select <code>Rebuild Container</code> or select <code>cmd + shift + P</code> (Mac) or <code>ctrl + shift + p</code> (Windows) and type <code>Remote-Containers: Rebuild Container</code>.</p>"},{"location":"tools/development/vscode/#rebuilding-a-dev-container-and-destroying-cache","title":"Rebuilding a Dev Container and Destroying Cache","text":"<p>To also destroy the cache while rebuilding a Dev Container, select <code>cmd + shift + P</code> (Mac) or <code>ctrl + shift + p</code> (Windows) and type <code>Remote-Containers: Rebuild Container Without Cache</code>. This will take appreciably longer but can help clear up stubborn issues with dependencies.</p>"},{"location":"tools/gitlab/compliance-pipelines/","title":"Compliance Pipelines","text":"<p>Compliance Pipelines are Gitlab CI pipelines that run on specific projects that have been added to a Compliance Framework. They, in turn, combine with the project's pipelines to cover test, build and deploy tasks, along with other compliance-related tasks.</p>"},{"location":"tools/gitlab/compliance-pipelines/#what-is-a-compliance-framework","title":"What is a Compliance Framework","text":"<p>A Compliance Framework, in brief, is a way to apply consistent, centralized, and enterprise-guided tasks to a large set of CI Pipelines. By applying a Compliance Framework to an individual project, team leadership can be assured that defined quality and security tasks are being universally performed, with all the up-flowing reporting and insight that goes along with.</p>"},{"location":"tools/gitlab/compliance-pipelines/#what-is-a-compliance-pipeline","title":"What is a Compliance Pipeline","text":"<p>A Compliance Pipeline, specifically, is the actual set of commands that define CI/CD stages, and the resulting pipeline runs. These pipelines are defined in a <code>yml</code> file, located inside a Gitlab repository, with all the syntactical options available in a regular <code>.gitlab-ci.yml</code>. The thing to keep in mind here is that a compliance pipeline, once enabled, supersedes the regular <code>.gitlab-ci.yml</code> file, and it cannot be overwritten. In fact, the compliance pipeline technically needs to call the local <code>.gitlab-ci.yml</code> file, or it will not run at all.</p> <p>Once enabled and configured, the compliance pipeline operates similarly to a regular CI file. Obviously it has different goals (you would not use a compliance pipeline for a build or promotion step, for example), but otherwise it is the same kind of thing; it can be used for tests, scans, reports, with the flexibility of calling different Docker images and executing the whole host of commands that might be useful or pertinent.</p>"},{"location":"tools/gitlab/compliance-pipelines/#quick-overview-for-developers","title":"Quick Overview for Developers","text":"<p>If you are a developer looking for more information on why there are several \"mystery\" stages showing up in your pipelines, this section is for you.</p> <p>In brief, this stages are being called in and executed by an extra <code>.gitlab-ci.yml</code> file located in a different place. These stages are all present for quality and/or security purposes, and you shouldn't ever need to alter them. They cannot be skipped or edited.</p> <p>The Compliance Framework is added to your project by an individual with Owner rights, and features a blue badge when active.</p> <p>If any of these stages fail for reasons that are not clear (meaning, something other than an actual code error that was detected in your project), please let the DevOps team know.</p>"},{"location":"tools/gitlab/compliance-pipelines/#quick-overview-for-devopsmaintainers","title":"Quick Overview for DevOps/Maintainers","text":"<p>In general, these pull in a selection of Gitlab-provided and custom templates to accomplish the variety of reporting and security tasks it implements. This one template is in use for all different varieties of projects, from TypeScript to Terraform. It uses simple Gitlab CI logic to check for the existence of relevant filetypes.</p> <p>As of this time, all Compliance Frameworks have been applied manually, using the UI, as this feature is not currently available in Gitlab's IaC platform. There is the possibility of applying by batch using Gitlab's GraphQL API, but it has not been executed yet.</p>"},{"location":"tools/gitlab/compliance-pipelines/#update-the-compliance-pipeline","title":"Update the Compliance Pipeline","text":"<p>In order to make updates to the pipeline (for example, to add an additional scan or remove an existing one), make the edit to the relevant <code>yml</code> file, either the <code>.compliance-ci.yml</code> file or the template it pulls in. Immediately after committing the change, it will be in force across all the projects that have the Compliance Framework activated.</p>"},{"location":"tools/gitlab/compliance-pipelines/#stages","title":"Stages","text":"<p>Because the Compliance Pipeline takes precedence over the individual project Gitlab CI, it is required that any stages that are configured in the individual <code>.gitlab-ci.yml</code> file are present in the Compliance Pipeline definition. These stages include</p> <ul> <li>.pre</li> <li>test</li> <li>build</li> <li>deploy</li> <li>.post</li> </ul>"},{"location":"tools/gitlab/compliance-pipelines/#output","title":"Output","text":"<p>Vulnerabilities will be reported on the Gitlab Vulnerability Report.</p>"},{"location":"tools/gitlab/gitlab-vulnerability-report/","title":"Gitlab Vulnerability Report","text":"<p>The Gitlab Vulnerability report takes the output of security scanners (either provided by Gitlab or custom-made) and presents the findings in a dashboard inside each project, and also collates the findings in each group.</p> <p>The vulnerability report will automatically post the findings of a scan if it is in the correct format, and run against the default branch of a repository. If a custom scanner is used, the output must be configured in the Gitlab CI file to place the output in the correct location (see example below), and be of the correct format. </p>"},{"location":"tools/gitlab/iac-pipeline-scanners/","title":"IaC Pipeline Scanners","text":"<p>As Infrastructure as Code (IaC) increases in complexity and scale, and provisions assets with sensitive material, it is critically important to have scanners that look for a wide variety of vulnerabilities at every stage of development and requisition. It isn't enough to scan at the time of writing the IaC code, or once the assets have been deployed to AWS. As such, we have developed strategies to scan the IaC locally, in the Gitlab Pipeline(s), and in production.</p> <p>!!! note     These scans can be added to Gitlab's Compliance Pipelines, which means that if your project is covered by the Compliance Framework, these scanners will run automatically.</p>"},{"location":"tools/gitlab/iac-pipeline-scanners/#custom-iac-scanners","title":"Custom IaC Scanners","text":""},{"location":"tools/gitlab/iac-pipeline-scanners/#gitlab-provided-scanning","title":"Gitlab-provided Scanning","text":"<p>Gitlab has a suite of built-in, freely available SAST scanners, one of which is specifically designed for IaC. It is activated in a Gitlab CI/CD pipeline by simply adding the following line to the <code>Include</code> block of a <code>.gitlab-ci.yml</code>:</p> <pre><code>include:\n  - template: Security/SAST-IaC.gitlab-ci.yml\n</code></pre> <p>!!! note     There is Gitlab documentation on the other SAST scanners available.</p> <p>The underlying scanning protocol is provided by KICS, which is a thorough scanning platform, but does not adhere to any specification beyond its own.</p>"},{"location":"tools/gitlab/iac-pipeline-scanners/#additional-scanning","title":"Additional Scanning","text":"<p>Specifically for IaC, there are a variety of addition scanning platforms beyond KICS, each of which has its own pros and cons. These include (but are not necessarily limited to):</p> <ul> <li>Snyk</li> <li>Checkov</li> <li>TFLint</li> <li>Terrascan</li> <li>TFSec</li> <li>Terraform Compliance</li> </ul> <p>At this time, we are utilizing several of these solutions to ensure that there are no gaps in coverage, and we put our IaC in the very best position to shift left and thusly reduce the complexity of problems that appear once infrastructure has been deployed.</p> <p>In order to activate one of these scans in your pipeline, similarly to the built-in Gitlab scan, simply include the reference to the template:</p> <p><code>yaml include:   - template: Security/SAST-IaC.latest.gitlab-ci.yml   - project: \"devops/helpers\"     ref: \"main\"     file: \"ci-templates/sast/iac/checkov.yml\"   - project: \"devops/helpers\"     ref: \"main\"     file: \"ci-templates/sast/iac/tflint.yml\"   - project: \"devops/helpers\"     ref: \"main\"     file: \"ci-templates/sast/iac/tfsec.yml\"</code></p> <p>This example will conduct all the steps consistent with these scanners.</p>"},{"location":"tools/gitlab/iac-pipeline-scanners/#custom-scanner-converters","title":"Custom Scanner Converters","text":"<p>The custom scanners reside in their own repository, and are fairly simple scripts written in Typescript (compiled to JavaScript) and designed to be run in a stage in Gitlab CI/CD pipeline. Because they are stand-alone JavaScript server-side scripts, they are designed to use an environment that has NodeJS installed. </p> <p>The reason to convert the scan's output is so that it will appear in Gitlab's built-in Security Dashboard, which is enabled as part of our service agreement. This way the default output (if enabled) and any custom output will appear in the same location, which will make tracking and remediation more streamlined and traceable. Currently there are scripts for Checkov and Snyk.</p>"}]}